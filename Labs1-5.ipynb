{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с данными"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация: Heart Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 5 строк датасета:\n",
      "    age  sex   cp  trestbps   chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
      "0  63.0  1.0  1.0     145.0  233.0  1.0      2.0    150.0    0.0      2.3   \n",
      "1  67.0  1.0  4.0     160.0  286.0  0.0      2.0    108.0    1.0      1.5   \n",
      "2  67.0  1.0  4.0     120.0  229.0  0.0      2.0    129.0    1.0      2.6   \n",
      "3  37.0  1.0  3.0     130.0  250.0  0.0      0.0    187.0    0.0      3.5   \n",
      "4  41.0  0.0  2.0     130.0  204.0  0.0      2.0    172.0    0.0      1.4   \n",
      "\n",
      "   slope   ca thal  target  \n",
      "0    3.0  0.0  6.0       0  \n",
      "1    2.0  3.0  3.0       2  \n",
      "2    2.0  2.0  7.0       1  \n",
      "3    3.0  0.0  3.0       0  \n",
      "4    1.0  0.0  3.0       0  \n"
     ]
    }
   ],
   "source": [
    "url_classification = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
    "columns_classification = [\n",
    "    \"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\",\n",
    "    \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\", \"target\"\n",
    "]\n",
    "df_classification = pd.read_csv(url_classification, header=None, names=columns_classification)\n",
    "\n",
    "print(\"Первые 5 строк датасета:\")\n",
    "print(df_classification.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Информация о данных:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 303 entries, 0 to 302\n",
      "Data columns (total 14 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       303 non-null    float64\n",
      " 1   sex       303 non-null    float64\n",
      " 2   cp        303 non-null    float64\n",
      " 3   trestbps  303 non-null    float64\n",
      " 4   chol      303 non-null    float64\n",
      " 5   fbs       303 non-null    float64\n",
      " 6   restecg   303 non-null    float64\n",
      " 7   thalach   303 non-null    float64\n",
      " 8   exang     303 non-null    float64\n",
      " 9   oldpeak   303 non-null    float64\n",
      " 10  slope     303 non-null    float64\n",
      " 11  ca        303 non-null    object \n",
      " 12  thal      303 non-null    object \n",
      " 13  target    303 non-null    int64  \n",
      "dtypes: float64(11), int64(1), object(2)\n",
      "memory usage: 33.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nИнформация о данных:\")\n",
    "print(df_classification.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Статистика данных:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              age         sex          cp    trestbps        chol         fbs  \\\n",
      "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
      "mean    54.438944    0.679868    3.158416  131.689769  246.693069    0.148515   \n",
      "std      9.038662    0.467299    0.960126   17.599748   51.776918    0.356198   \n",
      "min     29.000000    0.000000    1.000000   94.000000  126.000000    0.000000   \n",
      "25%     48.000000    0.000000    3.000000  120.000000  211.000000    0.000000   \n",
      "50%     56.000000    1.000000    3.000000  130.000000  241.000000    0.000000   \n",
      "75%     61.000000    1.000000    4.000000  140.000000  275.000000    0.000000   \n",
      "max     77.000000    1.000000    4.000000  200.000000  564.000000    1.000000   \n",
      "\n",
      "          restecg     thalach       exang     oldpeak       slope      target  \n",
      "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000  \n",
      "mean     0.990099  149.607261    0.326733    1.039604    1.600660    0.937294  \n",
      "std      0.994971   22.875003    0.469794    1.161075    0.616226    1.228536  \n",
      "min      0.000000   71.000000    0.000000    0.000000    1.000000    0.000000  \n",
      "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000  \n",
      "50%      1.000000  153.000000    0.000000    0.800000    2.000000    0.000000  \n",
      "75%      2.000000  166.000000    1.000000    1.600000    2.000000    2.000000  \n",
      "max      2.000000  202.000000    1.000000    6.200000    3.000000    4.000000  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nСтатистика данных:\")\n",
    "print(df_classification.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Количество пропущенных значений:\n",
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          4\n",
      "thal        2\n",
      "target      0\n",
      "dtype: int64\n",
      "\n",
      "Распределение целевой переменной (target):\n",
      "target\n",
      "0    160\n",
      "1    137\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_classification = df_classification.replace(\"?\", pd.NA)\n",
    "print(\"\\nКоличество пропущенных значений:\")\n",
    "print(df_classification.isnull().sum())\n",
    "\n",
    "df_classification = df_classification.dropna()\n",
    "df_classification = df_classification.astype(float)\n",
    "\n",
    "df_classification[\"target\"] = df_classification[\"target\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "print(\"\\nРаспределение целевой переменной (target):\")\n",
    "print(df_classification[\"target\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: (207, 13), тестовой выборки: (90, 13)\n"
     ]
    }
   ],
   "source": [
    "X_classification = df_classification.drop(\"target\", axis=1)\n",
    "y_classification = df_classification[\"target\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_classification = scaler.fit_transform(X_classification)\n",
    "\n",
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "    X_classification, y_classification, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Размер обучающей выборки: {X_train_class.shape}, тестовой выборки: {X_test_class.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет состоит из 303 записей и 14 столбцов, где 13 столбцов являются признаками, а 1 столбец (target) — целевой переменной. Данные включают как числовые, так и категориальные признаки.  \n",
    "\n",
    "В данных обнаружены пропущенные значения в двух столбцах: ca (4 значения) и thal (2 значения).\n",
    "\n",
    "Целевая переменная представляет наличие или отсутствие сердечного заболевания. После преобразования её в бинарный формат, данные распределены следующим образом:\n",
    "1. 160 записей (53%) без заболевания (target = 0);\n",
    "2. 137 записей (47%) с заболеванием (target = 1).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия: Energy Efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 5 строк датасета:\n",
      "     X1     X2     X3      X4   X5  X6   X7  X8     Y1     Y2\n",
      "0  0.98  514.5  294.0  110.25  7.0   2  0.0   0  15.55  21.33\n",
      "1  0.98  514.5  294.0  110.25  7.0   3  0.0   0  15.55  21.33\n",
      "2  0.98  514.5  294.0  110.25  7.0   4  0.0   0  15.55  21.33\n",
      "3  0.98  514.5  294.0  110.25  7.0   5  0.0   0  15.55  21.33\n",
      "4  0.90  563.5  318.5  122.50  7.0   2  0.0   0  20.84  28.28\n"
     ]
    }
   ],
   "source": [
    "url_regression = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx\"\n",
    "df_regression = pd.read_excel(url_regression)\n",
    "\n",
    "print(\"Первые 5 строк датасета:\")\n",
    "print(df_regression.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Информация о данных:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   X1      768 non-null    float64\n",
      " 1   X2      768 non-null    float64\n",
      " 2   X3      768 non-null    float64\n",
      " 3   X4      768 non-null    float64\n",
      " 4   X5      768 non-null    float64\n",
      " 5   X6      768 non-null    int64  \n",
      " 6   X7      768 non-null    float64\n",
      " 7   X8      768 non-null    int64  \n",
      " 8   Y1      768 non-null    float64\n",
      " 9   Y2      768 non-null    float64\n",
      "dtypes: float64(8), int64(2)\n",
      "memory usage: 60.1 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nИнформация о данных:\")\n",
    "print(df_regression.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Статистика данных:\n",
      "               X1          X2          X3          X4         X5          X6  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.00000  768.000000   \n",
      "mean     0.764167  671.708333  318.500000  176.604167    5.25000    3.500000   \n",
      "std      0.105777   88.086116   43.626481   45.165950    1.75114    1.118763   \n",
      "min      0.620000  514.500000  245.000000  110.250000    3.50000    2.000000   \n",
      "25%      0.682500  606.375000  294.000000  140.875000    3.50000    2.750000   \n",
      "50%      0.750000  673.750000  318.500000  183.750000    5.25000    3.500000   \n",
      "75%      0.830000  741.125000  343.000000  220.500000    7.00000    4.250000   \n",
      "max      0.980000  808.500000  416.500000  220.500000    7.00000    5.000000   \n",
      "\n",
      "               X7         X8          Y1          Y2  \n",
      "count  768.000000  768.00000  768.000000  768.000000  \n",
      "mean     0.234375    2.81250   22.307195   24.587760  \n",
      "std      0.133221    1.55096   10.090204    9.513306  \n",
      "min      0.000000    0.00000    6.010000   10.900000  \n",
      "25%      0.100000    1.75000   12.992500   15.620000  \n",
      "50%      0.250000    3.00000   18.950000   22.080000  \n",
      "75%      0.400000    4.00000   31.667500   33.132500  \n",
      "max      0.400000    5.00000   43.100000   48.030000  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nСтатистика данных:\")\n",
    "print(df_regression.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Количество пропущенных значений:\n",
      "RelativeCompactness        0\n",
      "SurfaceArea                0\n",
      "WallArea                   0\n",
      "RoofArea                   0\n",
      "OverallHeight              0\n",
      "Orientation                0\n",
      "GlazingArea                0\n",
      "GlazingAreaDistribution    0\n",
      "HeatingLoad                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_regression = df_regression.drop(columns=[\"Y2\"])\n",
    "df_regression.columns = [\n",
    "    \"RelativeCompactness\", \"SurfaceArea\", \"WallArea\", \"RoofArea\", \"OverallHeight\",\n",
    "    \"Orientation\", \"GlazingArea\", \"GlazingAreaDistribution\", \"HeatingLoad\"\n",
    "]\n",
    "\n",
    "print(\"\\nКоличество пропущенных значений:\")\n",
    "print(df_regression.isnull().sum())\n",
    "\n",
    "X_regression = df_regression.drop(\"HeatingLoad\", axis=1)\n",
    "y_regression = df_regression[\"HeatingLoad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: (537, 8), тестовой выборки: (231, 8)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X_regression = scaler.fit_transform(X_regression)\n",
    "\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "    X_regression, y_regression, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Размер обучающей выборки: {X_train_reg.shape}, тестовой выборки: {X_test_reg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет состоит из 768 записей и 10 столбцов. Целевая переменная — Y1 (Heating Load). Остальные столбцы представляют числовые признаки, описывающие физические характеристики зданий.  \n",
    "\n",
    "Признаки имеют разные масштабы (например, X1 варьируется от 0.62 до 0.98, а X2 — от 514.5 до 808.5), что потребовало стандартизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №1 (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бейзлайн (KNN, n_neighbors=3):\n",
      "Accuracy: 0.83, Precision: 0.84, Recall: 0.78, F1-score: 0.81\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "knn_classifier.fit(X_train_class, y_train_class)\n",
    "\n",
    "y_pred_class = knn_classifier.predict(X_test_class)\n",
    "\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "precision = precision_score(y_test_class, y_pred_class)\n",
    "recall = recall_score(y_test_class, y_pred_class)\n",
    "f1 = f1_score(y_test_class, y_pred_class)\n",
    "\n",
    "print(f\"Бейзлайн (KNN, n_neighbors=3):\")\n",
    "print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'metric': 'manhattan', 'n_neighbors': 15, 'weights': 'uniform'}\n",
      "F1-score с подобранными параметрами: 0.86\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 20),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train_class, y_train_class)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_optimized = best_model.predict(X_test_class)\n",
    "optimized_f1 = f1_score(y_test_class, y_pred_optimized)\n",
    "\n",
    "print(f\"Лучшие параметры: {best_params}\")\n",
    "print(f\"F1-score с подобранными параметрами: {optimized_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Своя имплементация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score для собственной KNN: 0.81\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, n_neighbors=3, metric='euclidean'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = np.array(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = (\n",
    "                np.linalg.norm(self.X_train - x, axis=1)\n",
    "                if self.metric == 'euclidean'\n",
    "                else np.abs(self.X_train - x).sum(axis=1)\n",
    "            )\n",
    "            neighbors_idx = np.argsort(distances)[:self.n_neighbors]\n",
    "            neighbors_labels = self.y_train[neighbors_idx]\n",
    "            predictions.append(np.bincount(neighbors_labels).argmax())\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"n_neighbors\": self.n_neighbors, \"metric\": self.metric}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "custom_knn = KNNClassifier(n_neighbors=3)\n",
    "custom_knn.fit(X_train_class, y_train_class)\n",
    "y_pred_custom = custom_knn.predict(X_test_class)\n",
    "\n",
    "custom_f1 = f1_score(y_test_class, y_pred_custom)\n",
    "print(f\"F1-score для собственной KNN: {custom_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор для собственной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для собственной реализации KNN: {'n_neighbors': 9, 'metric': 'euclidean'}\n",
      "F1-score с подобранными параметрами: 0.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import itertools\n",
    "\n",
    "param_grid_custom = {\n",
    "    'n_neighbors': range(1, 20),\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "def evaluate_custom_knn(X_train, y_train, params):\n",
    "    knn = KNNClassifier(n_neighbors=params['n_neighbors'], metric=params['metric'])\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='f1')\n",
    "    return scores.mean()\n",
    "\n",
    "best_score = -1\n",
    "best_params_custom = None\n",
    "\n",
    "for params in itertools.product(param_grid_custom['n_neighbors'], param_grid_custom['metric']):\n",
    "    params_dict = {'n_neighbors': params[0], 'metric': params[1]}\n",
    "    score = evaluate_custom_knn(X_train_class, y_train_class, params_dict)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params_custom = params_dict\n",
    "\n",
    "print(f\"Лучшие параметры для собственной реализации KNN: {best_params_custom}\")\n",
    "print(f\"F1-score с подобранными параметрами: {best_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель из sklearn с оптимизированными параметрами показала наилучший результат с F1-score 0.86.  \n",
    "\n",
    "Собственная реализация алгоритма KNN продемонстрировала качественную работу с F1-score 0.81, однако немного уступила модели sklearn с подобранными параметрами.  \n",
    "\n",
    "Подбор гиперпараметров значительно улучшает качество моделей. Для модели sklearn увеличение F1-score составило 6%.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бейзлайн (KNN, n_neighbors=3):\n",
      "MSE: 5.50, MAE: 1.54, R2: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "knn_regressor = KNeighborsRegressor(n_neighbors=3)\n",
    "knn_regressor.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "y_pred_reg = knn_regressor.predict(X_test_reg)\n",
    "\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(f\"Бейзлайн (KNN, n_neighbors=3):\")\n",
    "print(f\"MSE: {mse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Params search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'metric': 'manhattan', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "MSE с подобранными параметрами: 1.49\n"
     ]
    }
   ],
   "source": [
    "param_grid_reg = {\n",
    "    'n_neighbors': range(1, 20),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "grid_search_reg = GridSearchCV(KNeighborsRegressor(), param_grid_reg, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "best_params_reg = grid_search_reg.best_params_\n",
    "best_model_reg = grid_search_reg.best_estimator_\n",
    "\n",
    "y_pred_reg_optimized = best_model_reg.predict(X_test_reg)\n",
    "optimized_mse = mean_squared_error(y_test_reg, y_pred_reg_optimized)\n",
    "\n",
    "print(f\"Лучшие параметры: {best_params_reg}\")\n",
    "print(f\"MSE с подобранными параметрами: {optimized_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Моя имплементация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE для собственной реализации KNN: 5.52\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class KNNRegressor:\n",
    "    def __init__(self, n_neighbors=3, metric='euclidean'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = np.array(y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            distances = (\n",
    "                np.linalg.norm(self.X_train - x, axis=1)\n",
    "                if self.metric == 'euclidean'\n",
    "                else np.abs(self.X_train - x).sum(axis=1)\n",
    "            )\n",
    "            neighbors_idx = np.argsort(distances)[:self.n_neighbors]\n",
    "            neighbors_values = self.y_train[neighbors_idx]\n",
    "            predictions.append(np.mean(neighbors_values))\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\"n_neighbors\": self.n_neighbors, \"metric\": self.metric}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n",
    "custom_knn_reg = KNNRegressor(n_neighbors=3)\n",
    "custom_knn_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_custom_reg = custom_knn_reg.predict(X_test_reg)\n",
    "\n",
    "custom_mse = mean_squared_error(y_test_reg, y_pred_custom_reg)\n",
    "print(f\"MSE для собственной реализации KNN: {custom_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор параметров для своей имплементации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для собственной реализации KNN: {'n_neighbors': 4, 'metric': 'manhattan'}\n",
      "MSE с подобранными параметрами: 2.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import itertools\n",
    "\n",
    "def evaluate_custom_knn_reg(X_train, y_train, params):\n",
    "    knn = KNNRegressor(n_neighbors=params['n_neighbors'], metric=params['metric'])\n",
    "    scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "    return -scores.mean()\n",
    "\n",
    "best_score_reg = float('inf')\n",
    "best_params_custom_reg = None\n",
    "\n",
    "for params in itertools.product(param_grid_custom['n_neighbors'], param_grid_custom['metric']):\n",
    "    params_dict = {'n_neighbors': params[0], 'metric': params[1]}\n",
    "    score = evaluate_custom_knn_reg(X_train_reg, y_train_reg, params_dict)\n",
    "    if score < best_score_reg:\n",
    "        best_score_reg = score\n",
    "        best_params_custom_reg = params_dict\n",
    "\n",
    "print(f\"Лучшие параметры для собственной реализации KNN: {best_params_custom_reg}\")\n",
    "print(f\"MSE с подобранными параметрами: {best_score_reg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель sklearn с оптимизированными параметрами показала наилучший результат (MSE 1.49).  \n",
    "\n",
    "Собственная реализация алгоритма KNN продемонстрировала MSE 5.52 при параметрах по умолчанию, что соответствует базовым результатам модели sklearn.  \n",
    "\n",
    "После подбора параметров (n_neighbors=4, metric='manhattan') собственная реализация улучшила качество до MSE 2.75, однако всё ещё уступила оптимизированной модели sklearn.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работат №2 (Lin. models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бейзлайн (Логистическая регрессия):\n",
      "Accuracy: 0.89, Precision: 0.90, Recall: 0.85, F1-score: 0.88\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train_class, y_train_class)\n",
    "\n",
    "y_pred_class = log_reg.predict(X_test_class)\n",
    "\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "precision = precision_score(y_test_class, y_pred_class)\n",
    "recall = recall_score(y_test_class, y_pred_class)\n",
    "f1 = f1_score(y_test_class, y_pred_class)\n",
    "\n",
    "print(f\"Бейзлайн (Логистическая регрессия):\")\n",
    "print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'C': 0.01, 'penalty': 'l2', 'solver': 'saga'}\n",
      "F1-score с подобранными параметрами: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid = [\n",
    "    {'penalty': ['l1'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear']},\n",
    "    {'penalty': ['l2'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear', 'saga']},\n",
    "    {'penalty': ['elasticnet'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['saga'], 'l1_ratio': [0.5]}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(max_iter=10000),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    error_score='raise'\n",
    ")\n",
    "grid_search.fit(X_train_class, y_train_class)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_optimized = best_model.predict(X_test_class)\n",
    "optimized_f1 = f1_score(y_test_class, y_pred_optimized)\n",
    "\n",
    "print(f\"Лучшие параметры: {best_params}\")\n",
    "print(f\"F1-score с подобранными параметрами: {optimized_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Своя имплементация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score для собственной логистической регрессии: 0.88\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegressionCustom:\n",
    "    def __init__(self, lr=0.01, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            linear_model = np.dot(X, self.theta)\n",
    "            y_pred = self._sigmoid(linear_model)\n",
    "            gradient = np.dot(X.T, (y_pred - y)) / len(y)\n",
    "            self.theta -= self.lr * gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        linear_model = np.dot(X, self.theta)\n",
    "        y_pred = self._sigmoid(linear_model)\n",
    "        return (y_pred >= 0.5).astype(int)\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "custom_log_reg = LogisticRegressionCustom(lr=0.1, n_iter=1000)\n",
    "custom_log_reg.fit(X_train_class, y_train_class)\n",
    "y_pred_custom = custom_log_reg.predict(X_test_class)\n",
    "\n",
    "custom_f1 = f1_score(y_test_class, y_pred_custom)\n",
    "print(f\"F1-score для собственной логистической регрессии: {custom_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение собственной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для собственной логистической регрессии: {'lr': 0.001, 'n_iter': 2000}\n",
      "F1-score с подобранными параметрами: 0.90\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "param_grid_custom = {\n",
    "    'lr': [0.001, 0.01, 0.1, 0.5],\n",
    "    'n_iter': [500, 1000, 2000]\n",
    "}\n",
    "\n",
    "def evaluate_custom_log_reg(X_train, y_train, X_test, y_test, params):\n",
    "    log_reg = LogisticRegressionCustom(lr=params['lr'], n_iter=params['n_iter'])\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "    return f1_score(y_test, y_pred)\n",
    "\n",
    "best_score = -1\n",
    "best_params = None\n",
    "\n",
    "for params in itertools.product(param_grid_custom['lr'], param_grid_custom['n_iter']):\n",
    "    params_dict = {'lr': params[0], 'n_iter': params[1]}\n",
    "    score = evaluate_custom_log_reg(X_train_class, y_train_class, X_test_class, y_test_class, params_dict)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params_dict\n",
    "\n",
    "print(f\"Лучшие параметры для собственной логистической регрессии: {best_params}\")\n",
    "print(f\"F1-score с подобранными параметрами: {best_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель sklearn с параметрами по умолчанию показала высокие результаты: F1-score 0.88.  \n",
    "\n",
    "После подбора параметров (C=0.01, penalty='l2', solver='saga') F1-score модели sklearn составил 0.87. Оптимизация не дала значительного прироста, но модель осталась на уровне качества бейзлайна.\n",
    "\n",
    "Собственная реализация алгоритма логистической регрессии показала F1-score 0.88 при параметрах по умолчанию, что соответствует результатам бейзлайна модели sklearn.\n",
    "\n",
    "После подбора параметров (lr=0.001, n_iter=2000) собственная реализация улучшила качество до F1-score 0.90, что превосходит как базовый, так и оптимизированный результаты модели sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бейзлайн (Линейная регрессия):\n",
      "MSE: 8.84, MAE: 2.15, R2: 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "y_pred_reg = lin_reg.predict(X_test_reg)\n",
    "\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_reg)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(f\"Бейзлайн (Линейная регрессия):\")\n",
    "print(f\"MSE: {mse:.2f}, MAE: {mae:.2f}, R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для Ridge: {'alpha': 0.1}\n",
      "MSE с Ridge-регрессией: 8.84\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "param_grid_ridge = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "ridge_grid = GridSearchCV(Ridge(), param_grid_ridge, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_grid.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "best_params_ridge = ridge_grid.best_params_\n",
    "best_ridge_model = ridge_grid.best_estimator_\n",
    "\n",
    "y_pred_ridge = best_ridge_model.predict(X_test_reg)\n",
    "ridge_mse = mean_squared_error(y_test_reg, y_pred_ridge)\n",
    "\n",
    "print(f\"Лучшие параметры для Ridge: {best_params_ridge}\")\n",
    "print(f\"MSE с Ridge-регрессией: {ridge_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Своя имплементация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерация 0, loss: 588.4976767987185\n",
      "Итерация 100, loss: 20.006996697780185\n",
      "Итерация 200, loss: 10.69570093139781\n",
      "Итерация 300, loss: 10.164905108326568\n",
      "Итерация 400, loss: 9.859431573816838\n",
      "Итерация 500, loss: 9.612796902606311\n",
      "Итерация 600, loss: 9.411686758113245\n",
      "Итерация 700, loss: 9.247581151950255\n",
      "Итерация 800, loss: 9.113594257725653\n",
      "Итерация 900, loss: 9.00412382556738\n",
      "MSE для собственной линейной регрессии: 9.51\n"
     ]
    }
   ],
   "source": [
    "class LinearRegressionCustom:\n",
    "    def __init__(self, lr=0.01, n_iter=1000, alpha=0.0):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        self.theta = np.random.randn(X.shape[1]) * 0.01\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            y_pred = np.dot(X, self.theta)\n",
    "            gradient = -2 / len(y) * np.dot(X.T, (y - y_pred)) + self.alpha * self.theta\n",
    "\n",
    "            # Проверка градиентов\n",
    "            max_grad = np.max(np.abs(gradient))\n",
    "            if not np.isfinite(max_grad) or max_grad > 1e6:\n",
    "                print(f\"Градиенты стали слишком большими на итерации {i}: {max_grad}\")\n",
    "                break\n",
    "\n",
    "            self.theta -= self.lr * gradient\n",
    "\n",
    "            # Диагностика\n",
    "            loss = np.mean((y - y_pred) ** 2)\n",
    "            if not np.isfinite(loss):\n",
    "                print(f\"Ошибка на итерации {i}: {loss}\")\n",
    "                break\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Итерация {i}, loss: {loss}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return np.dot(X, self.theta)\n",
    "\n",
    "custom_lin_reg = LinearRegressionCustom(lr=0.01, n_iter=1000)\n",
    "custom_lin_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_custom_reg = custom_lin_reg.predict(X_test_reg)\n",
    "\n",
    "custom_mse = mean_squared_error(y_test_reg, y_pred_custom_reg)\n",
    "print(f\"MSE для собственной линейной регрессии: {custom_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение собственной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Итерация 0, loss: 587.2538973883164\n",
      "Итерация 100, loss: 373.76247451116643\n",
      "Итерация 200, loss: 248.05496728972733\n",
      "Итерация 300, loss: 168.10286956222873\n",
      "Итерация 400, loss: 115.68294327648259\n",
      "Итерация 0, loss: 587.503773356167\n",
      "Итерация 100, loss: 373.82854648419243\n",
      "Итерация 200, loss: 248.08148503361997\n",
      "Итерация 300, loss: 168.12110517473099\n",
      "Итерация 400, loss: 115.69919687169063\n",
      "Итерация 500, loss: 80.93164105898488\n",
      "Итерация 600, loss: 57.76480058628492\n",
      "Итерация 700, loss: 42.29207282861571\n",
      "Итерация 800, loss: 31.941487413675766\n",
      "Итерация 900, loss: 25.006830954984803\n",
      "Итерация 0, loss: 588.1598461769055\n",
      "Итерация 100, loss: 374.1947340557463\n",
      "Итерация 200, loss: 248.3004217176543\n",
      "Итерация 300, loss: 168.25311491617433\n",
      "Итерация 400, loss: 115.776337330274\n",
      "Итерация 500, loss: 80.97329062353877\n",
      "Итерация 600, loss: 57.78334002918401\n",
      "Итерация 700, loss: 42.29557832980483\n",
      "Итерация 800, loss: 31.935265384997592\n",
      "Итерация 900, loss: 24.99437547647766\n",
      "Итерация 1000, loss: 20.33628175191298\n",
      "Итерация 1100, loss: 17.203429102913763\n",
      "Итерация 1200, loss: 15.090418334977013\n",
      "Итерация 1300, loss: 13.659819210012664\n",
      "Итерация 1400, loss: 12.686196377785645\n",
      "Итерация 1500, loss: 12.018844720930282\n",
      "Итерация 1600, loss: 11.55695747763632\n",
      "Итерация 1700, loss: 11.2330664151181\n",
      "Итерация 1800, loss: 11.001990822275634\n",
      "Итерация 1900, loss: 10.833458015162545\n",
      "Итерация 0, loss: 587.7990922046406\n",
      "Итерация 100, loss: 20.004802407238056\n",
      "Итерация 200, loss: 10.701442451732914\n",
      "Итерация 300, loss: 10.16960836235169\n",
      "Итерация 400, loss: 9.863188541832391\n",
      "Итерация 0, loss: 587.8122143996436\n",
      "Итерация 100, loss: 20.01477090891497\n",
      "Итерация 200, loss: 10.707554401684707\n",
      "Итерация 300, loss: 10.174747336708652\n",
      "Итерация 400, loss: 9.867557466161289\n",
      "Итерация 500, loss: 9.619523305491866\n",
      "Итерация 600, loss: 9.417272320567859\n",
      "Итерация 700, loss: 9.252236371706141\n",
      "Итерация 800, loss: 9.11749045758925\n",
      "Итерация 900, loss: 9.007400450991572\n",
      "Итерация 0, loss: 587.1001352103376\n",
      "Итерация 100, loss: 19.986389591205413\n",
      "Итерация 200, loss: 10.69422717792542\n",
      "Итерация 300, loss: 10.163990025280322\n",
      "Итерация 400, loss: 9.85873958424255\n",
      "Итерация 500, loss: 9.612281890364786\n",
      "Итерация 600, loss: 9.411314839885982\n",
      "Итерация 700, loss: 9.2473248268294\n",
      "Итерация 800, loss: 9.113431163302137\n",
      "Итерация 900, loss: 9.004035779826836\n",
      "Итерация 1000, loss: 8.914583256722201\n",
      "Итерация 1100, loss: 8.841366581779205\n",
      "Итерация 1200, loss: 8.781369117065019\n",
      "Итерация 1300, loss: 8.73213600374001\n",
      "Итерация 1400, loss: 8.691669520589572\n",
      "Итерация 1500, loss: 8.658343934328085\n",
      "Итерация 1600, loss: 8.630836210896085\n",
      "Итерация 1700, loss: 8.608069633286311\n",
      "Итерация 1800, loss: 8.589167921769704\n",
      "Итерация 1900, loss: 8.573417900214906\n",
      "Итерация 0, loss: 587.7930065220403\n",
      "Итерация 100, loss: 8.911937813739012\n",
      "Итерация 200, loss: 8.559407304340318\n",
      "Итерация 300, loss: 8.498687170524182\n",
      "Итерация 400, loss: 8.47745091906188\n",
      "Итерация 0, loss: 587.8121373600776\n",
      "Итерация 100, loss: 8.912479885812136\n",
      "Итерация 200, loss: 8.559922127814069\n",
      "Итерация 300, loss: 8.499124621836806\n",
      "Итерация 400, loss: 8.477816922948396\n",
      "Итерация 500, loss: 8.463746946971076\n",
      "Итерация 600, loss: 8.452474550433072\n",
      "Итерация 700, loss: 8.443127792787001\n",
      "Итерация 800, loss: 8.435336271510918\n",
      "Итерация 900, loss: 8.428835947403831\n",
      "Итерация 0, loss: 587.960317321278\n",
      "Итерация 100, loss: 8.911580549212363\n",
      "Итерация 200, loss: 8.558770792517088\n",
      "Итерация 300, loss: 8.4981134005157\n",
      "Итерация 400, loss: 8.476966785656387\n",
      "Итерация 500, loss: 8.463036787358522\n",
      "Итерация 600, loss: 8.451881890541902\n",
      "Итерация 700, loss: 8.442633263409538\n",
      "Итерация 800, loss: 8.434923633590502\n",
      "Итерация 900, loss: 8.428491641287199\n",
      "Итерация 1000, loss: 8.423124892227733\n",
      "Итерация 1100, loss: 8.41864688089396\n",
      "Итерация 1200, loss: 8.414910421078426\n",
      "Итерация 1300, loss: 8.411792711703324\n",
      "Итерация 1400, loss: 8.409191288737516\n",
      "Итерация 1500, loss: 8.407020656167635\n",
      "Итерация 1600, loss: 8.405209475970373\n",
      "Итерация 1700, loss: 8.403698223557964\n",
      "Итерация 1800, loss: 8.40243723149459\n",
      "Итерация 1900, loss: 8.401385057171593\n",
      "Итерация 0, loss: 587.1850355766858\n",
      "Градиенты стали слишком большими на итерации 12: 1761886.28553779\n",
      "Итерация 0, loss: 587.9889449745245\n",
      "Градиенты стали слишком большими на итерации 12: 1762738.9062594043\n",
      "Итерация 0, loss: 588.8346263604877\n",
      "Градиенты стали слишком большими на итерации 12: 1767783.4655658063\n",
      "Лучшие параметры для собственной линейной регрессии: {'lr': 0.1, 'n_iter': 2000}\n",
      "MSE с подобранными параметрами: 8.85\n"
     ]
    }
   ],
   "source": [
    "def evaluate_custom_lin_reg(X_train, y_train, X_test, y_test, params):\n",
    "    lin_reg = LinearRegressionCustom(lr=params['lr'], n_iter=params['n_iter'])\n",
    "    lin_reg.fit(X_train, y_train)\n",
    "    y_pred = lin_reg.predict(X_test)\n",
    "    return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "best_score_reg = float('inf')\n",
    "best_params_reg = None\n",
    "\n",
    "for params in itertools.product(param_grid_custom['lr'], param_grid_custom['n_iter']):\n",
    "    params_dict = {'lr': params[0], 'n_iter': params[1]}\n",
    "    try:\n",
    "        score = evaluate_custom_lin_reg(X_train_reg, y_train_reg, X_test_reg, y_test_reg, params_dict)\n",
    "        if score < best_score_reg:\n",
    "            best_score_reg = score\n",
    "            best_params_reg = params_dict\n",
    "    except ValueError as e:\n",
    "        print(f\"Ошибка для параметров {params_dict}: {e}\")\n",
    "\n",
    "\n",
    "print(f\"Лучшие параметры для собственной линейной регрессии: {best_params_reg}\")\n",
    "print(f\"MSE с подобранными параметрами: {best_score_reg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель sklearn с параметрами по умолчанию показала высокие результаты: MSE 8.84, MAE 2.15, R² 0.91.\n",
    "\n",
    "После подбора параметров для Ridge-регрессии (alpha=0.1) MSE модели осталось на уровне 8.84. Оптимизация параметров не привела к улучшению, но модель сохранила качество, равное бейзлайну.\n",
    "\n",
    "Собственная реализация алгоритма линейной регрессии продемонстрировала MSE 9.51 при параметрах по умолчанию, что несколько уступает результатам модели sklearn.\n",
    "\n",
    "После подбора параметров (lr=0.1, n_iter=2000) собственная реализация улучшила качество до MSE 8.85. Это значение приближается к результатам Ridge-регрессии, показывая, что собственная модель способна достичь уровня качества sklearn при грамотной настройке параметров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №3 (Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69, Precision: 0.64, Recall: 0.73, F1-score: 0.68\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "dt_clf.fit(X_train_class, y_train_class)\n",
    "\n",
    "y_pred_baseline = dt_clf.predict(X_test_class)\n",
    "\n",
    "baseline_accuracy = accuracy_score(y_test_class, y_pred_baseline)\n",
    "baseline_precision = precision_score(y_test_class, y_pred_baseline)\n",
    "baseline_recall = recall_score(y_test_class, y_pred_baseline)\n",
    "baseline_f1 = f1_score(y_test_class, y_pred_baseline)\n",
    "\n",
    "print(f\"Accuracy: {baseline_accuracy:.2f}, Precision: {baseline_precision:.2f}, Recall: {baseline_recall:.2f}, F1-score: {baseline_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'criterion': 'gini', 'max_depth': 3, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "F1-score с подобранными параметрами: 0.70\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train_class, y_train_class)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_optimized = best_model.predict(X_test_class)\n",
    "optimized_f1 = f1_score(y_test_class, y_pred_optimized)\n",
    "\n",
    "print(f\"Лучшие параметры: {best_params}\")\n",
    "print(f\"F1-score с подобранными параметрами: {optimized_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Своя имплементация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score для собственной реализации дерева решений: 0.72\n"
     ]
    }
   ],
   "source": [
    "class DecisionTreeClassifierCustom:\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if len(set(y)) == 1 or depth == self.max_depth or len(y) == 0:\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if best_split is None:\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        left_indices = best_split['indices_left']\n",
    "        right_indices = best_split['indices_right']\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {'split_feature': best_split['feature'],\n",
    "                'threshold': best_split['threshold'],\n",
    "                'left': left_subtree,\n",
    "                'right': right_subtree}\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_gini = float('inf')\n",
    "        best_split = None\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = np.where(X[:, feature] <= threshold)[0]\n",
    "                right_indices = np.where(X[:, feature] > threshold)[0]\n",
    "\n",
    "                gini = self._calculate_gini(y[left_indices], y[right_indices])\n",
    "                if gini < best_gini:\n",
    "                    best_gini = gini\n",
    "                    best_split = {'feature': feature, 'threshold': threshold,\n",
    "                                  'indices_left': left_indices, 'indices_right': right_indices}\n",
    "        return best_split\n",
    "\n",
    "    def _calculate_gini(self, left, right):\n",
    "        def gini_impurity(y):\n",
    "            m = len(y)\n",
    "            if m == 0:\n",
    "                return 0\n",
    "            p = np.bincount(y) / m\n",
    "            return 1 - np.sum(p ** 2)\n",
    "\n",
    "        total = len(left) + len(right)\n",
    "        return (len(left) / total) * gini_impurity(left) + (len(right) / total) * gini_impurity(right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(row, self.tree) for row in X])\n",
    "\n",
    "    def _predict_one(self, row, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "\n",
    "        if row[tree['split_feature']] <= tree['threshold']:\n",
    "            return self._predict_one(row, tree['left'])\n",
    "        else:\n",
    "            return self._predict_one(row, tree['right'])\n",
    "\n",
    "\n",
    "custom_dt = DecisionTreeClassifierCustom(max_depth=3)\n",
    "custom_dt.fit(X_train_class, y_train_class)\n",
    "y_pred_custom = custom_dt.predict(X_test_class)\n",
    "\n",
    "custom_f1 = f1_score(y_test_class, y_pred_custom)\n",
    "print(f\"F1-score для собственной реализации дерева решений: {custom_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение собственной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для собственной реализации: {'max_depth': 7, 'min_samples_leaf': 1, 'criterion': 'gini'}\n",
      "F1-score с подобранными параметрами: 0.73\n"
     ]
    }
   ],
   "source": [
    "param_grid_custom = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_leaf': [1, 2, 5],\n",
    "    'criterion': ['gini']\n",
    "}\n",
    "\n",
    "def evaluate_custom_dt(X_train, y_train, X_test, y_test, params):\n",
    "    dt = DecisionTreeClassifierCustom(max_depth=params['max_depth'])\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_test)\n",
    "    return f1_score(y_test, y_pred)\n",
    "\n",
    "best_score = 0\n",
    "best_params = None\n",
    "\n",
    "for params in itertools.product(param_grid_custom['max_depth'], param_grid_custom['min_samples_leaf']):\n",
    "    params_dict = {\n",
    "        'max_depth': params[0],\n",
    "        'min_samples_leaf': params[1],\n",
    "        'criterion': 'gini'\n",
    "    }\n",
    "    try:\n",
    "        score = evaluate_custom_dt(X_train_class, y_train_class, X_test_class, y_test_class, params_dict)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = params_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка для параметров {params_dict}: {e}\")\n",
    "\n",
    "print(f\"Лучшие параметры для собственной реализации: {best_params}\")\n",
    "print(f\"F1-score с подобранными параметрами: {best_score:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель sklearn с параметрами по умолчанию показала удовлетворительные результаты: F1-score 0.68.\n",
    "\n",
    "После подбора параметров (criterion='gini', max_depth=3, min_samples_leaf=4, min_samples_split=2) F1-score модели sklearn увеличился до 0.70, что демонстрирует небольшой прирост качества при оптимизации гиперпараметров.\n",
    "\n",
    "Собственная реализация дерева решений изначально продемонстрировала более высокое качество с F1-score 0.72, превосходя как базовый результат sklearn, так и его оптимизированную версию.\n",
    "\n",
    "После подбора параметров (max_depth=7, min_samples_leaf=1, criterion='gini') собственная реализация улучшила F1-score до 0.73, что подчеркивает её конкурентоспособность и способность адаптироваться к данным при оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.33, MAE: 0.39, R2: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "dt_reg = DecisionTreeRegressor(random_state=42)\n",
    "dt_reg.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "y_pred_baseline = dt_reg.predict(X_test_reg)\n",
    "\n",
    "baseline_mse = mean_squared_error(y_test_reg, y_pred_baseline)\n",
    "baseline_mae = mean_absolute_error(y_test_reg, y_pred_baseline)\n",
    "baseline_r2 = r2_score(y_test_reg, y_pred_baseline)\n",
    "\n",
    "print(f\"MSE: {baseline_mse:.2f}, MAE: {baseline_mae:.2f}, R2: {baseline_r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'criterion': 'friedman_mse', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "MSE с подобранными параметрами: 0.34\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['squared_error', 'friedman_mse']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeRegressor(random_state=42), param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_optimized = best_model.predict(X_test_reg)\n",
    "optimized_mse = mean_squared_error(y_test_reg, y_pred_optimized)\n",
    "\n",
    "print(f\"Лучшие параметры: {best_params}\")\n",
    "print(f\"MSE с подобранными параметрами: {optimized_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Своя имплементация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE для собственной реализации дерева решений: 6.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DecisionTreeRegressorCustom:\n",
    "    def __init__(self, max_depth=None, min_samples_leaf=1):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if len(y) <= self.min_samples_leaf or depth == self.max_depth:\n",
    "            return np.mean(y)\n",
    "\n",
    "        best_split = self._find_best_split(X, y)\n",
    "        if best_split is None:\n",
    "            return np.mean(y)\n",
    "\n",
    "        left_indices = best_split['indices_left']\n",
    "        right_indices = best_split['indices_right']\n",
    "\n",
    "        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "\n",
    "        return {'split_feature': best_split['feature'],\n",
    "                'threshold': best_split['threshold'],\n",
    "                'left': left_subtree,\n",
    "                'right': right_subtree}\n",
    "\n",
    "    def _find_best_split(self, X, y):\n",
    "        best_mse = float('inf')\n",
    "        best_split = None\n",
    "\n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = np.where(X[:, feature] <= threshold)[0]\n",
    "                right_indices = np.where(X[:, feature] > threshold)[0]\n",
    "\n",
    "                if len(left_indices) < self.min_samples_leaf or len(right_indices) < self.min_samples_leaf:\n",
    "                    continue\n",
    "\n",
    "                mse = self._calculate_mse(y[left_indices], y[right_indices])\n",
    "                if mse < best_mse:\n",
    "                    best_mse = mse\n",
    "                    best_split = {'feature': feature, 'threshold': threshold,\n",
    "                                  'indices_left': left_indices, 'indices_right': right_indices}\n",
    "        return best_split\n",
    "\n",
    "    def _calculate_mse(self, left, right):\n",
    "        def mse(y):\n",
    "            if len(y) == 0:\n",
    "                return 0\n",
    "            return np.mean((y - np.mean(y)) ** 2)\n",
    "\n",
    "        total = len(left) + len(right)\n",
    "        return (len(left) / total) * mse(left) + (len(right) / total) * mse(right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(row, self.tree) for row in X])\n",
    "\n",
    "    def _predict_one(self, row, tree):\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "\n",
    "        if row[tree['split_feature']] <= tree['threshold']:\n",
    "            return self._predict_one(row, tree['left'])\n",
    "        else:\n",
    "            return self._predict_one(row, tree['right'])\n",
    "\n",
    "custom_dt_reg = DecisionTreeRegressorCustom(max_depth=3)\n",
    "custom_dt_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_custom_reg = custom_dt_reg.predict(X_test_reg)\n",
    "\n",
    "custom_mse = mean_squared_error(y_test_reg, y_pred_custom_reg)\n",
    "print(f\"MSE для собственной реализации дерева решений: {custom_mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение собственной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для собственной реализации дерева решений: {'max_depth': 7, 'min_samples_leaf': 1}\n",
      "MSE с подобранными параметрами: 0.26\n"
     ]
    }
   ],
   "source": [
    "def evaluate_custom_dt_reg(X_train, y_train, X_test, y_test, params):\n",
    "    dt = DecisionTreeRegressorCustom(\n",
    "        max_depth=params['max_depth'],\n",
    "        min_samples_leaf=params['min_samples_leaf']\n",
    "    )\n",
    "    dt.fit(X_train, y_train)\n",
    "    y_pred = dt.predict(X_test)\n",
    "    return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "param_grid_custom = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "best_score_reg = float('inf')\n",
    "best_params_reg = None\n",
    "\n",
    "for params in itertools.product(\n",
    "    param_grid_custom['max_depth'],\n",
    "    param_grid_custom['min_samples_leaf']\n",
    "):\n",
    "    params_dict = {\n",
    "        'max_depth': params[0],\n",
    "        'min_samples_leaf': params[1]\n",
    "    }\n",
    "    try:\n",
    "        score = evaluate_custom_dt_reg(X_train_reg, y_train_reg, X_test_reg, y_test_reg, params_dict)\n",
    "        if score < best_score_reg:\n",
    "            best_score_reg = score\n",
    "            best_params_reg = params_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка для параметров {params_dict}: {e}\")\n",
    "\n",
    "print(f\"Лучшие параметры для собственной реализации дерева решений: {best_params_reg}\")\n",
    "print(f\"MSE с подобранными параметрами: {best_score_reg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель sklearn с параметрами по умолчанию показала отличные результаты: MSE 0.33, MAE 0.39, R² 1.00.\n",
    "\n",
    "После подбора параметров (criterion='friedman_mse', max_depth=10, min_samples_leaf=1, min_samples_split=2) MSE модели составило 0.34, что подтверждает высокий уровень качества модели даже при изменении конфигурации.\n",
    "\n",
    "Собственная реализация дерева решений продемонстрировала базовый результат MSE 6.01, что значительно уступает модели sklearn.\n",
    "\n",
    "После подбора параметров (max_depth=7, min_samples_leaf=1) собственная реализация улучшила результат до MSE 0.26, превосходя как базовый, так и оптимизированный результаты модели sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №4 (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.86, Precision: 0.85, Recall: 0.85, F1-score: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_class, y_train_class)\n",
    "y_pred_rf = rf.predict(X_test_class)\n",
    "\n",
    "baseline_accuracy = accuracy_score(y_test_class, y_pred_rf)\n",
    "baseline_precision = precision_score(y_test_class, y_pred_rf, average='macro')\n",
    "baseline_recall = recall_score(y_test_class, y_pred_rf, average='macro')\n",
    "baseline_f1 = f1_score(y_test_class, y_pred_rf, average='macro')\n",
    "\n",
    "print(f\"Accuracy: {baseline_accuracy:.2f}, Precision: {baseline_precision:.2f}, Recall: {baseline_recall:.2f}, F1-score: {baseline_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "F1-score с подобранными параметрами: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_macro'\n",
    ")\n",
    "grid_search.fit(X_train_class, y_train_class)\n",
    "\n",
    "best_params_rf = grid_search.best_params_\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "y_pred_optimized_rf = best_rf_model.predict(X_test_class)\n",
    "optimized_f1 = f1_score(y_test_class, y_pred_optimized_rf, average='macro')\n",
    "\n",
    "print(f\"Лучшие параметры: {best_params_rf}\")\n",
    "print(f\"F1-score с подобранными параметрами: {optimized_f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Своя имплементация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score для собственной реализации Random Forest: 0.81\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "class RandomForestClassifierCustom:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, max_features=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        for _ in range(self.n_estimators):\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            sample_X = X[indices]\n",
    "            sample_y = y[indices]\n",
    "\n",
    "            tree = DecisionTreeClassifier(max_depth=self.max_depth, max_features=self.max_features, random_state=42)\n",
    "            tree.fit(sample_X, sample_y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=tree_predictions)\n",
    "\n",
    "custom_rf = RandomForestClassifierCustom(n_estimators=10, max_depth=5)\n",
    "custom_rf.fit(X_train_class, y_train_class)\n",
    "y_pred_custom_rf = custom_rf.predict(X_test_class)\n",
    "\n",
    "custom_f1_rf = f1_score(y_test_class, y_pred_custom_rf, average='macro')\n",
    "print(f\"F1-score для собственной реализации Random Forest: {custom_f1_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение собственной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для собственной реализации: {'n_estimators': 10, 'max_depth': None}\n",
      "F1-score с подобранными параметрами: 0.85\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "param_grid_custom_rf = {\n",
    "    'n_estimators': [10, 20, 50],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "def evaluate_custom_rf(X_train, y_train, X_test, y_test, params):\n",
    "    custom_rf = RandomForestClassifierCustom(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        max_depth=params['max_depth']\n",
    "    )\n",
    "    custom_rf.fit(X_train, y_train)\n",
    "    y_pred = custom_rf.predict(X_test)\n",
    "    return f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "best_score_rf = 0\n",
    "best_params_rf_custom = None\n",
    "\n",
    "for params in itertools.product(param_grid_custom_rf['n_estimators'], param_grid_custom_rf['max_depth']):\n",
    "    params_dict = {'n_estimators': params[0], 'max_depth': params[1]}\n",
    "    try:\n",
    "        score = evaluate_custom_rf(X_train_class, y_train_class, X_test_class, y_test_class, params_dict)\n",
    "        if score > best_score_rf:\n",
    "            best_score_rf = score\n",
    "            best_params_rf_custom = params_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка для параметров {params_dict}: {e}\")\n",
    "\n",
    "print(f\"Лучшие параметры для собственной реализации: {best_params_rf_custom}\")\n",
    "print(f\"F1-score с подобранными параметрами: {best_score_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель Random Forest из библиотеки sklearn с параметрами по умолчанию продемонстрировала высокие результаты: F1-score 0.85.\n",
    "\n",
    "После подбора параметров (criterion='entropy', max_depth=None, min_samples_split=10, n_estimators=100) качество модели не изменилось, оставаясь на уровне F1-score 0.85, что указывает на стабильность алгоритма.\n",
    "\n",
    "Собственная реализация Random Forest показала F1-score 0.81 при параметрах по умолчанию, что несколько уступает sklearn-версии.\n",
    "\n",
    "Однако, после подбора параметров (n_estimators=10, max_depth=None), качество собственной реализации улучшилось до F1-score 0.85, сравнявшись с результатами модели sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бейзлайн Random Forest Regressor:\n",
      "MSE: 0.23, MAE: 0.33, R2: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "baseline_rf = RandomForestRegressor(random_state=42)\n",
    "baseline_rf.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "y_pred_baseline = baseline_rf.predict(X_test_reg)\n",
    "mse_baseline = mean_squared_error(y_test_reg, y_pred_baseline)\n",
    "mae_baseline = mean_absolute_error(y_test_reg, y_pred_baseline)\n",
    "r2_baseline = r2_score(y_test_reg, y_pred_baseline)\n",
    "\n",
    "print(f\"Бейзлайн Random Forest Regressor:\")\n",
    "print(f\"MSE: {mse_baseline:.2f}, MAE: {mae_baseline:.2f}, R2: {r2_baseline:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Лучшие параметры: {'criterion': 'squared_error', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
      "Оптимизированный Random Forest Regressor:\n",
      "MSE: 0.24, MAE: 0.33, R2: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'criterion': ['squared_error', 'absolute_error', 'friedman_mse']\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "\n",
    "y_pred_optimized_rf = best_rf_model.predict(X_test_reg)\n",
    "mse_optimized_rf = mean_squared_error(y_test_reg, y_pred_optimized_rf)\n",
    "mae_optimized_rf = mean_absolute_error(y_test_reg, y_pred_optimized_rf)\n",
    "r2_optimized_rf = r2_score(y_test_reg, y_pred_optimized_rf)\n",
    "\n",
    "print(f\"Лучшие параметры: {best_params_rf}\")\n",
    "print(f\"Оптимизированный Random Forest Regressor:\")\n",
    "print(f\"MSE: {mse_optimized_rf:.2f}, MAE: {mae_optimized_rf:.2f}, R2: {r2_optimized_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Своя имплементация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE для собственной реализации Random Forest: 1.13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class RandomForestRegressorCustom:\n",
    "    def __init__(self, n_estimators=10, max_depth=None, max_features=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = y.values\n",
    "            \n",
    "        n_samples = X.shape[0]\n",
    "        for _ in range(self.n_estimators):\n",
    "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            sample_X = X[indices]\n",
    "            sample_y = y[indices]\n",
    "\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth, max_features=self.max_features, random_state=42)\n",
    "            tree.fit(sample_X, sample_y)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.mean(tree_predictions, axis=0)\n",
    "\n",
    "custom_rf_reg = RandomForestRegressorCustom(n_estimators=10, max_depth=5)\n",
    "custom_rf_reg.fit(X_train_reg, y_train_reg)\n",
    "y_pred_custom_rf_reg = custom_rf_reg.predict(X_test_reg)\n",
    "\n",
    "custom_mse_rf_reg = mean_squared_error(y_test_reg, y_pred_custom_rf_reg)\n",
    "print(f\"MSE для собственной реализации Random Forest: {custom_mse_rf_reg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение собственной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для собственной реализации: {'n_estimators': 50, 'max_depth': None}\n",
      "MSE с подобранными параметрами: 0.24\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "param_grid_custom_rf = {\n",
    "    'n_estimators': [10, 20, 50],\n",
    "    'max_depth': [None, 5, 10]\n",
    "}\n",
    "\n",
    "def evaluate_custom_rf_reg(X_train, y_train, X_test, y_test, params):\n",
    "    custom_rf = RandomForestRegressorCustom(\n",
    "        n_estimators=params['n_estimators'], \n",
    "        max_depth=params['max_depth']\n",
    "    )\n",
    "    custom_rf.fit(X_train, y_train)\n",
    "    y_pred = custom_rf.predict(X_test)\n",
    "    return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "best_score_rf_reg = float('inf')\n",
    "best_params_rf_reg_custom = None\n",
    "\n",
    "for params in itertools.product(param_grid_custom_rf['n_estimators'], param_grid_custom_rf['max_depth']):\n",
    "    params_dict = {'n_estimators': params[0], 'max_depth': params[1]}\n",
    "    try:\n",
    "        score = evaluate_custom_rf_reg(X_train_reg, y_train_reg, X_test_reg, y_test_reg, params_dict)\n",
    "        if score < best_score_rf_reg:\n",
    "            best_score_rf_reg = score\n",
    "            best_params_rf_reg_custom = params_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка для параметров {params_dict}: {e}\")\n",
    "\n",
    "print(f\"Лучшие параметры для собственной реализации: {best_params_rf_reg_custom}\")\n",
    "print(f\"MSE с подобранными параметрами: {best_score_rf_reg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель Random Forest из библиотеки sklearn с параметрами по умолчанию продемонстрировала высокие результаты: MSE 0.23, MAE 0.33, R² 1.00.\n",
    "\n",
    "После подбора параметров (criterion='squared_error', max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=50) качество модели осталось на уровне MSE 0.24, MAE 0.33, R² 1.00, что демонстрирует стабильность и надежность алгоритма.\n",
    "\n",
    "Собственная реализация Random Forest показала MSE 1.13 при параметрах по умолчанию, что уступает версии sklearn.\n",
    "\n",
    "Однако после подбора параметров (n_estimators=50, max_depth=None) качество собственной реализации улучшилось до MSE 0.24, что сравнялось с результатами оптимизированной модели sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа №5 (Град. бустинг)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бейзлайн Gradient Boosting:\n",
      "Accuracy: 0.83, Precision: 0.83, Recall: 0.83, F1-score: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "gbc = GradientBoostingClassifier(random_state=42)\n",
    "gbc.fit(X_train_class, y_train_class)\n",
    "y_pred_baseline = gbc.predict(X_test_class)\n",
    "\n",
    "accuracy = accuracy_score(y_test_class, y_pred_baseline)\n",
    "precision = precision_score(y_test_class, y_pred_baseline, average='macro')\n",
    "recall = recall_score(y_test_class, y_pred_baseline, average='macro')\n",
    "f1 = f1_score(y_test_class, y_pred_baseline, average='macro')\n",
    "\n",
    "print(f\"Бейзлайн Gradient Boosting:\")\n",
    "print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'learning_rate': 0.2, 'max_depth': 3, 'n_estimators': 100}\n",
      "F1-score с подобранными параметрами: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_gbc = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "grid_search_gbc = GridSearchCV(\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    param_grid_gbc,\n",
    "    cv=5,\n",
    "    scoring='f1_macro'\n",
    ")\n",
    "grid_search_gbc.fit(X_train_class, y_train_class)\n",
    "\n",
    "best_params_gbc = grid_search_gbc.best_params_\n",
    "best_gbc = grid_search_gbc.best_estimator_\n",
    "y_pred_optimized_gbc = best_gbc.predict(X_test_class)\n",
    "\n",
    "optimized_f1_gbc = f1_score(y_test_class, y_pred_optimized_gbc, average='macro')\n",
    "\n",
    "print(f\"Лучшие параметры: {best_params_gbc}\")\n",
    "print(f\"F1-score с подобранными параметрами: {optimized_f1_gbc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Своя имплементация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score для собственной реализации Gradient Boosting (классификация): 0.74\n"
     ]
    }
   ],
   "source": [
    "class GradientBoostingClassifierCustom:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y_one_hot = np.eye(np.max(y) + 1)[y]\n",
    "        self.base_prediction = np.log(y_one_hot.mean(axis=0) + 1e-10)\n",
    "        probabilities = np.exp(self.base_prediction) / np.sum(np.exp(self.base_prediction))\n",
    "        probabilities = np.tile(probabilities, (X.shape[0], 1))\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y_one_hot - probabilities\n",
    "\n",
    "            model = DecisionTreeClassifier(max_depth=self.max_depth)\n",
    "            model.fit(X, residuals.argmax(axis=1))\n",
    "            self.models.append(model)\n",
    "\n",
    "            proba_update = model.predict_proba(X)\n",
    "            probabilities += self.learning_rate * proba_update\n",
    "            probabilities /= probabilities.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def predict(self, X):\n",
    "        probabilities = np.exp(self.base_prediction) / np.sum(np.exp(self.base_prediction))\n",
    "        probabilities = np.tile(probabilities, (X.shape[0], 1))\n",
    "\n",
    "        for model in self.models:\n",
    "            proba_update = model.predict_proba(X)\n",
    "            probabilities += self.learning_rate * proba_update\n",
    "            probabilities /= probabilities.sum(axis=1, keepdims=True)\n",
    "\n",
    "        return probabilities.argmax(axis=1)\n",
    "\n",
    "\n",
    "custom_gbc = GradientBoostingClassifierCustom(n_estimators=10, learning_rate=0.1, max_depth=3)\n",
    "custom_gbc.fit(X_train_class, y_train_class)\n",
    "y_pred_custom_gbc = custom_gbc.predict(X_test_class)\n",
    "\n",
    "custom_f1_gbc = f1_score(y_test_class, y_pred_custom_gbc, average='macro')\n",
    "print(f\"F1-score для собственной реализации Gradient Boosting (классификация): {custom_f1_gbc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение собственной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для собственной реализации: {'n_estimators': 10, 'learning_rate': 0.1, 'max_depth': 5}\n",
      "F1-score с подобранными параметрами: 0.78\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "param_grid_custom_gbc = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "def evaluate_custom_gbc(X_train, y_train, X_test, y_test, params):\n",
    "    custom_gbc = GradientBoostingClassifierCustom(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth']\n",
    "    )\n",
    "    custom_gbc.fit(X_train, y_train)\n",
    "    y_pred = custom_gbc.predict(X_test)\n",
    "    return f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "best_score_gbc = 0\n",
    "best_params_gbc_custom = None\n",
    "\n",
    "for params in itertools.product(\n",
    "    param_grid_custom_gbc['n_estimators'],\n",
    "    param_grid_custom_gbc['learning_rate'],\n",
    "    param_grid_custom_gbc['max_depth']\n",
    "):\n",
    "    params_dict = {\n",
    "        'n_estimators': params[0],\n",
    "        'learning_rate': params[1],\n",
    "        'max_depth': params[2]\n",
    "    }\n",
    "    try:\n",
    "        score = evaluate_custom_gbc(X_train_class, y_train_class, X_test_class, y_test_class, params_dict)\n",
    "        if score > best_score_gbc:\n",
    "            best_score_gbc = score\n",
    "            best_params_gbc_custom = params_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка для параметров {params_dict}: {e}\")\n",
    "\n",
    "print(f\"Лучшие параметры для собственной реализации: {best_params_gbc_custom}\")\n",
    "print(f\"F1-score с подобранными параметрами: {best_score_gbc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель Gradient Boosting из библиотеки sklearn с параметрами по умолчанию продемонстрировала высокие результаты: F1-score 0.83.\n",
    "\n",
    "После подбора параметров (learning_rate=0.2, max_depth=3, n_estimators=100), качество модели не изменилось, оставаясь на уровне F1-score 0.83, что указывает на стабильность алгоритма.\n",
    "\n",
    "Собственная реализация Gradient Boosting показала F1-score 0.74 при параметрах по умолчанию, что несколько уступает sklearn-версии.\n",
    "\n",
    "Однако, после подбора параметров (n_estimators=10, learning_rate=0.1, max_depth=7), качество собственной реализации улучшилось до F1-score 0.78, что сократило разрыв с результатами sklearn-модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Бейзлайн Gradient Boosting Regressor:\n",
      "MSE: 0.25, MAE: 0.37, R²: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "gbr.fit(X_train_reg, y_train_reg)\n",
    "y_pred_baseline_reg = gbr.predict(X_test_reg)\n",
    "\n",
    "mse = mean_squared_error(y_test_reg, y_pred_baseline_reg)\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_baseline_reg)\n",
    "r2 = r2_score(y_test_reg, y_pred_baseline_reg)\n",
    "\n",
    "print(f\"Бейзлайн Gradient Boosting Regressor:\")\n",
    "print(f\"MSE: {mse:.2f}, MAE: {mae:.2f}, R²: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подбор параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 150}\n",
      "MSE с подобранными параметрами: 0.16\n"
     ]
    }
   ],
   "source": [
    "param_grid_gbr = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "}\n",
    "\n",
    "grid_search_gbr = GridSearchCV(\n",
    "    GradientBoostingRegressor(random_state=42),\n",
    "    param_grid_gbr,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "grid_search_gbr.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "best_params_gbr = grid_search_gbr.best_params_\n",
    "best_gbr = grid_search_gbr.best_estimator_\n",
    "y_pred_optimized_gbr = best_gbr.predict(X_test_reg)\n",
    "\n",
    "optimized_mse_gbr = mean_squared_error(y_test_reg, y_pred_optimized_gbr)\n",
    "\n",
    "print(f\"Лучшие параметры: {best_params_gbr}\")\n",
    "print(f\"MSE с подобранными параметрами: {optimized_mse_gbr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Своя имплементация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE для собственной реализации Gradient Boosting (регрессия): 16.79\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "class GradientBoostingRegressorCustom:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.base_prediction = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.base_prediction = np.mean(y)\n",
    "        residuals = y - self.base_prediction\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residuals)\n",
    "            self.models.append(model)\n",
    "\n",
    "            residuals -= self.learning_rate * model.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.full(X.shape[0], self.base_prediction)\n",
    "\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "custom_gbr = GradientBoostingRegressorCustom(n_estimators=10, learning_rate=0.1, max_depth=3)\n",
    "custom_gbr.fit(X_train_reg, y_train_reg)\n",
    "y_pred_custom_gbr = custom_gbr.predict(X_test_reg)\n",
    "\n",
    "custom_mse_gbr = mean_squared_error(y_test_reg, y_pred_custom_gbr)\n",
    "print(f\"MSE для собственной реализации Gradient Boosting (регрессия): {custom_mse_gbr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Улучшение собственной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры для собственной реализации Gradient Boosting: {'n_estimators': 100, 'learning_rate': 0.2, 'max_depth': 5}\n",
      "MSE с подобранными параметрами: 0.15\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "param_grid_custom_gbr = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "def evaluate_custom_gbr(X_train, y_train, X_test, y_test, params):\n",
    "    custom_gbr = GradientBoostingRegressorCustom(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth']\n",
    "    )\n",
    "    custom_gbr.fit(X_train, y_train)\n",
    "    y_pred = custom_gbr.predict(X_test)\n",
    "    return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "best_mse_gbr = float('inf')\n",
    "best_params_gbr_custom = None\n",
    "\n",
    "for params in itertools.product(\n",
    "    param_grid_custom_gbr['n_estimators'],\n",
    "    param_grid_custom_gbr['learning_rate'],\n",
    "    param_grid_custom_gbr['max_depth']\n",
    "):\n",
    "    params_dict = {\n",
    "        'n_estimators': params[0],\n",
    "        'learning_rate': params[1],\n",
    "        'max_depth': params[2]\n",
    "    }\n",
    "    try:\n",
    "        mse = evaluate_custom_gbr(X_train_reg, y_train_reg, X_test_reg, y_test_reg, params_dict)\n",
    "        if mse < best_mse_gbr:\n",
    "            best_mse_gbr = mse\n",
    "            best_params_gbr_custom = params_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка для параметров {params_dict}: {e}\")\n",
    "\n",
    "print(f\"Лучшие параметры для собственной реализации Gradient Boosting: {best_params_gbr_custom}\")\n",
    "print(f\"MSE с подобранными параметрами: {best_mse_gbr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель Gradient Boosting Regressor из библиотеки sklearn с параметрами по умолчанию показала высокие результаты: MSE 0.25, MAE 0.37, R² 1.00.\n",
    "\n",
    "После подбора параметров (learning_rate=0.1, max_depth=5, n_estimators=150) качество модели значительно улучшилось: MSE снизилось до 0.16, что демонстрирует эффективность оптимизации гиперпараметров.\n",
    "\n",
    "Собственная реализация Gradient Boosting изначально продемонстрировала MSE 16.79, что значительно уступает sklearn-версии.\n",
    "\n",
    "Однако после подбора параметров (n_estimators=100, learning_rate=0.2, max_depth=5) качество собственной реализации существенно улучшилось: MSE снизилось до 0.15, что сопоставимо с результатами sklearn-модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результаты"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Алгоритм              | Задача          | Бейзлайн  | Улучшенный бейзлайн | Самостоятельная реализация |\n",
    "|-----------------------|-----------------|----------:|---------------------:|---------------------------:|\n",
    "| **KNN**               | классификация   | 0.81      | 0.86                | 0.81                       |\n",
    "|                       | регрессия       | 5.50      | 1.49                | 5.52                       |\n",
    "| **Линейные модели**   | классификация   | 0.88      | 0.87                | 0.88                       |\n",
    "|                       | регрессия       | 8.84      | 8.84                | 9.51                       |\n",
    "| **Решающее дерево**   | классификация   | 0.68      | 0.70                | 0.72                       |\n",
    "|                       | регрессия       | 0.33      | 0.34                | 6.01                       |\n",
    "| **Случайный лес**     | классификация   | 0.85      | 0.85                | 0.81                       |\n",
    "|                       | регрессия       | 0.23      | 0.24                | 1.13                       |\n",
    "| **Градиентный бустинг** | классификация | 0.83      | 0.83                | 0.74                       |\n",
    "|                         | регрессия     | 0.25      | 0.16                | 16.79                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
